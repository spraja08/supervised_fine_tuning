# Supervised FineTuning

## A small language model (Phi-2) with a custom dataset

This is an attempt to create a specialist model that can be trained within a few dollars but can produce surprising good results. Phi-2 seems to have "seen" most commercially  available content datasets. To convincingly test the "specialisation effect", I had to hunt for a less common dataset. Huggingface came handy with such a variety of datasets that can be readily used without much dataprep. 

The results came to be as good as I could expect. The codes in the fine_bitsy.ipynb in this repo !

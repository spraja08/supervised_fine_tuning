{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4727adb-7646-4397-aaf8-f1d8a702b540",
   "metadata": {
    "id": "XIyP_0r6zuVc"
   },
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"./model_finetuning_banner.jpeg\" width=\"100%\">\n",
    "\n",
    "# Fine-tuning : Phi-2 using QLoRA with a custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b517cd3-4868-4961-9812-bc6746d4e1fc",
   "metadata": {
    "id": "E2CkxsA43m15"
   },
   "source": [
    "### 1. Prepare the environment\n",
    "\n",
    "The combination of the model and the data I used does not need more than 1 GPU but would need more than 32GB GPU VRAM. I used [runpod.io's](https://www.runpod.io/console/gpu-cloud) instance that packed it right for me - 1 A100 GPU with 80GB VRAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1feb7b-fa55-48b5-ab64-8822ae97875a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets einops\n",
    "!pip install -q -U matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38daafe0-aae7-4f8e-954d-2dbd0397d290",
   "metadata": {
    "id": "05H5MIfjyRgc"
   },
   "source": [
    "### 2. Accelerator / W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc11aa4-42bf-4082-ab1c-067c400e5ca0",
   "metadata": {
    "id": "TEzYBadkyRgd"
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cb0d0-1063-4691-8244-6dce1733f517",
   "metadata": {
    "id": "-9KNTJZkyRgn"
   },
   "source": [
    "Using the Weights & Biases to track our training metrics. Very useful to check the evaluation losses graphically and select the optimum checkpoint for the final use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a71a47-4e80-43fe-82a8-d8140525dcf8",
   "metadata": {
    "id": "DDqUNyIoyRgo"
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"fed-res-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be378315-c391-4f7f-8db3-b1442387cc62",
   "metadata": {
    "id": "QcE4NTeFyRgd"
   },
   "source": [
    "### 3. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece42f7c-3825-45c7-9afc-efb355e9474c",
   "metadata": {
    "id": "FCc64bfnmd3j"
   },
   "source": [
    "It was difficult to choose the dataset as Phi-2's original model seems to have \"seen\" pretty much all the popular datasets. Needed something that's a bit obscure and chose this [federal reserve question answers](https://huggingface.co/datasets/clement-cvll/us-federal-reserve-qa/viewer) dataset from hugging faces. It is very tiny but I rather overfit my model on this and see if I could really influence phi-2 deterministically. Usually, I split the data into train,val,test sets. But in this exercise, I will use the same data for train and val as the dataset is too small and I want to try overfitting my model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab8440-a2c5-4450-9095-5a18da3e80b9",
   "metadata": {
    "id": "EmZbX-ltyRge",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"clement-cvll/us-federal-reserve-qa\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"clement-cvll/us-federal-reserve-qa\", split=\"train\")\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97961855-bec0-4b3b-ab0e-c013a329de75",
   "metadata": {
    "id": "shz8Xdv-yRgf"
   },
   "source": [
    "### 4. Load Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26071032-245a-4305-85f2-c2eda775d626",
   "metadata": {},
   "source": [
    "Load Phi-2, quantized !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb1be3-7408-46db-b38b-9fe517c85741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"microsoft/phi-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b01769f1-79a0-4f97-9867-32f065e14751",
   "metadata": {
    "id": "UjNdXolqyRgf"
   },
   "source": [
    "### 5. Tokenization\n",
    "\n",
    "`max_length`,  has a direct impact on the compute requirements. Can compute this but I want to plot and check visually. Setting up the  tokenizer without the truncation/padding to get the length distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a7057-ed43-42c0-a3dd-3e69515aa709",
   "metadata": {
    "id": "WLvc85zDyRgg"
   },
   "source": [
    "Setup the tokenize function to make labels and input_ids the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e689a62-d15a-4da6-9c69-0d4c7020e979",
   "metadata": {
    "id": "1hFsEFp5yRgg"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True, \n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    ")\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(prompt)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86958cc-9443-4b56-adb4-540f97aad84b",
   "metadata": {
    "id": "tJtsbrr6yRgg"
   },
   "source": [
    "And convert each sample into the prompt format. I am using the following format as mentoned in [phi-2's huggingfaces documentation](https://huggingface.co/microsoft/phi-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16792cd-cfbb-4503-9af9-32560af83ec4",
   "metadata": {
    "id": "6z9rvnoDyRgg"
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"Instruction:{data_point[\"Context\"]}\n",
    "    Assistant:{data_point[\"Response\"]}\"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9329f2-054c-4dd3-b446-e2483210dcaa",
   "metadata": {
    "id": "WHnKLcq4yRgg"
   },
   "source": [
    "Reformat the prompt and tokenize each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ba8cc-7766-4300-95ab-96605fed5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a489de-9060-4c37-91ea-160caf3ec01c",
   "metadata": {},
   "source": [
    "Untokenizing to make sure it was formatted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f9d42-0315-413d-99a2-1745f94159ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_text = tokenizer.decode(tokenized_train_dataset[0]['input_ids']) \n",
    "print(untokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe974dc-4d06-4a61-a5e5-db191c92c965",
   "metadata": {},
   "source": [
    "Plot the distribution of the dataset lengths, so we can determine the appropriate `max_length` for our input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a94ba3-95d1-4e53-bb03-96215f68e126",
   "metadata": {
    "id": "BA8M9yfC3m19",
    "outputId": "99c6d302-9bb6-47b1-cae9-a1cd870b4770",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23cc8d-68f2-4230-8a75-330a490d39b0",
   "metadata": {},
   "source": [
    "I am going to keep the max length = 250 approximately from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293be61-6b90-4da7-933e-1679a1d1220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250 \n",
    "\n",
    "# redefine the tokenize function and tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,  \n",
    "    add_bos_token=True,  \n",
    "    trust_remote_code=True,\n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238ada4-5ed9-4970-a594-932663f77282",
   "metadata": {
    "id": "TQL796OayRgh"
   },
   "source": [
    "Verify if each `input_ids` is padded on the left with the `eos_token` (50256) and there should be an `eos_token` 50256 added to the end, and the prompt should start with a `bos_token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b915cd9-d26f-44c4-95ce-d979b34f60be",
   "metadata": {
    "id": "-CRrG-SkyRgh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenized_train_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ea587-9d43-48e3-8d6a-6c8bdb0a96e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "untokenized_text = tokenizer.decode(tokenized_train_dataset[4]['input_ids']) \n",
    "print(untokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ed0fc-3b57-4523-b352-e70da097af74",
   "metadata": {
    "id": "I6LRa2Zm3m19"
   },
   "source": [
    "Checking to see if all the training data should be the same length, `max_length` (250 in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01676d8f-4e82-48a6-a3ce-279651574983",
   "metadata": {
    "id": "I55Yo3yy3m19",
    "outputId": "c87e344d-e0f3-4542-afcc-4e2025926d64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2eb79-e9b8-4d29-9667-7be9bc3368b0",
   "metadata": {
    "id": "7fi9wEZYyRgh"
   },
   "source": [
    "#### How does the model respond before SFT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f21bb8-e2df-4d76-bec4-bcbe966310c8",
   "metadata": {
    "id": "k_VRZDh9yRgi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Context: \" + eval_dataset[0]['Context'])\n",
    "print(\"Response: \" + eval_dataset[0]['Response'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5020387-45b8-4a77-a63e-15cf6b1d8d5a",
   "metadata": {
    "id": "gOxnx-cAyRgi"
   },
   "outputs": [],
   "source": [
    "eval_prompt = full_prompt =f\"\"\"Instruction:What should I do if I have damaged or mutilated currency?\n",
    "    Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9866d9f-0578-4a61-8b13-f100a1a344ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d3b8e-88a6-4fbd-9375-509ea9a296af",
   "metadata": {
    "id": "NidIuFXMyRgi"
   },
   "outputs": [],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    use_fast=False, # needed for now, should be fixed soon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a253a4-a3a8-43b3-abb2-d602d8fa2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f9452-0016-48f7-b355-588907eaff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=128)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc6c58-8338-4d5d-a8c0-05dfd7162423",
   "metadata": {
    "id": "dCAWeCzZyRgi"
   },
   "source": [
    "That is not the expected response but phi-2 was convincingly fluent :)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c776579-4e4e-45d8-8c96-62cfb4293211",
   "metadata": {
    "id": "AapDoyfAyRgi"
   },
   "source": [
    "### 6. Set Up LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682baafc-f687-493b-90fd-a7f384bad549",
   "metadata": {
    "id": "Mp2gMi1ZzGET"
   },
   "source": [
    "Preprocessing to the model to prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c07b7-4670-4974-81c9-7c2dfc7a1d10",
   "metadata": {
    "id": "gkIcwsSU01EB"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f088a21-62b6-46e6-9323-2aa583754f4b",
   "metadata": {
    "id": "cUYEpEK-yRgj"
   },
   "source": [
    "Examining the model's layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e477004-dbdb-4feb-82a0-681289522fdf",
   "metadata": {
    "id": "XshGNsbxyRgj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630857be-801a-4586-a562-88ffc7772058",
   "metadata": {
    "id": "I6mTLuQJyRgj"
   },
   "source": [
    "Define the LoRA config. (To play with this laterc with variations in r and alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ebe71-0bf2-4cdc-a20c-2a61bcebee1a",
   "metadata": {
    "id": "Ybeyl20n3dYH"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"Wqkv\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9a52d-4c54-4f9b-8485-94a5033fe6d4",
   "metadata": {
    "id": "X_FHi_VLyRgn"
   },
   "source": [
    "Model with LoRA adapters added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fdc11-ca63-4cac-8097-d533cedbd533",
   "metadata": {
    "id": "IaYMWak4yRgn"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4c7fe-c05a-479a-9208-84d86d22c0bf",
   "metadata": {
    "id": "_0MOtwf3zdZp"
   },
   "source": [
    "### 7. Run Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7599e90-27c6-461a-acfc-3c3a1e77cf4c",
   "metadata": {
    "id": "fEe0uWYSyRgo"
   },
   "source": [
    "Preparing the SFT parameters. I am choosing 1000 steps. I could go more to overfit the data which, depending on the use case, is probably the right thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0aae4-60e2-4853-a949-1d2026c66e98",
   "metadata": {
    "id": "c_L1131GyRgo"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: \n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832143f1-35a3-454c-82f9-42f195a03c8f",
   "metadata": {
    "id": "jq0nX33BmfaC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"fed-res-finetune\"\n",
    "base_model_name = \"phi2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, \n",
    "        logging_steps=25,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        \n",
    "        save_strategy=\"steps\",       \n",
    "        save_steps=50,                \n",
    "        evaluation_strategy=\"steps\", \n",
    "        eval_steps=50,               \n",
    "        do_eval=True,                \n",
    "        report_to=\"wandb\",           \n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"         \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806802ac-eab6-4f22-a048-bd153e6745e1",
   "metadata": {},
   "source": [
    "I will use the checkpoint at step 950 as it gave me the lowest val loss so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24beb8e2-1ea7-4c30-a8cc-37ff6b6e62b0",
   "metadata": {
    "id": "0D57XqcsyRgo"
   },
   "source": [
    "### 8. Playing with the FineTuned Model\n",
    "\n",
    "Load the base Phi-2 model from the Huggingface again and merge the qlora adapters generated by peft in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdefc4-8b5b-4c16-82ff-ae54b70a50b4",
   "metadata": {
    "id": "GwsiqhWuyRgp"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"phi2-fed-res-finetune/checkpoint-950\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240eaf08-96f9-434c-8d3a-a77939eaeab8",
   "metadata": {
    "id": "lMkVNEUvyRgp"
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"What should I do if I have damaged or mutilated currency?\"\n",
    "#eval_prompt = \"Who is on the Federal Open Market Committee?\"\n",
    "#eval_prompt = \"\"\"What does the Federal Reserve mean when it says monetary policy remains \"accommodative\"?\"\"\"\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=80)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582c22a-3d76-49c6-be34-4cd386a23c6c",
   "metadata": {
    "id": "VCJnpZoayRgq"
   },
   "source": [
    "###    That worked. The responses are pretty close to the actual custom dataset used. Indeed awesome, compared to the responses from the pre-finetuned version    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817f920-cbfc-45ea-b571-0b56ad5cdeb8",
   "metadata": {},
   "source": [
    "### 9. Pushing the adapters to huggingface hub\n",
    "\n",
    "Push (only) the qlora adapters generated by peft to the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd19ac2-f4d4-4270-b228-81cea801f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"token-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53435c5f-b3bc-44ae-90c3-eae2ac7cb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.push_to_hub(\"spraja08/fine-bitsy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb7c51-ded1-425f-9163-a3020bc91cc3",
   "metadata": {},
   "source": [
    "### 10. Loading from the hub and inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6bb12e-41e0-4648-8e7e-f75b9f918acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig\n",
    "peft_model_id_from_hub = \"spraja08/fine-bitsy\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id_from_hub)\n",
    "model_from_hub = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7c5c5-74cd-4b9f-9f82-d08cbd042dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_hub.eval()\n",
    "eval_prompt = \"\"\"Instruction:What should I do if I have damaged or mutilated currency?\n",
    "Assistant:\"\"\"\n",
    "#eval_prompt = \"Who is on the Federal Open Market Committee?\"\n",
    "#eval_prompt = \"\"\"What does the Federal Reserve mean when it says monetary policy remains \"accommodative\"?\"\"\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model_from_hub.generate(**model_input, max_new_tokens=80)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
